{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Week 1 Practical Exercise: Clean and Tokenize**"
      ],
      "metadata": {
        "id": "MnVnuAeVc44y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, You will clean a mini-corpus of 1 000 Amazon reviews, apply three different tokenisers, collect simple coverage metrics, and decide which approach best suits this dataset."
      ],
      "metadata": {
        "id": "KRSd4zYTdRSn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SECTIONÂ 0Â â€”Â Imports & Dataset** -------------------------------------------------\n",
        "We pull 1â€¯000 Amazon reviews from the HuggingÂ Face Hub.  Feel free to\n",
        "replace with a local CSV by editing the `load_dataset` call."
      ],
      "metadata": {
        "id": "Pys_GK9IdnSZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2y1oINBnc4LZ"
      },
      "outputs": [],
      "source": [
        "!pip -q install --upgrade datasets unidecode sentencepiece spacy numpy pandas\n",
        "\n",
        "from datasets import load_dataset\n",
        "from unidecode import unidecode\n",
        "import re, html, unicodedata, random, json\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "# Download a 1â€¯000â€‘example slice (balanced pos/neg)\n",
        "raw_ds = load_dataset(\"amazon_polarity\", split=\"train[:1000]\")\n",
        "print(\"Loaded\", len(raw_ds), \"Amazon reviews\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SECTIONÂ 1Â â€”Â Cleaning helpers**  (âœ…Â already implemented)"
      ],
      "metadata": {
        "id": "CCZFQ6J_dz9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "URL_RE   = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
        "EMOJI_RE = re.compile(r\"[\\U00010000-\\U0010ffff]\", flags=re.UNICODE)\n",
        "\n",
        "def clean_text(text: str, *, keep_emoji: bool = True) -> str:\n",
        "    \"\"\"Basic cleaner: HTML â†’ NFC â†’ lowercase â†’ optional emoji strip.\"\"\"\n",
        "    text = html.unescape(text)\n",
        "    text = unicodedata.normalize(\"NFC\", text)\n",
        "    text = URL_RE.sub(\"\", text)\n",
        "    if not keep_emoji:\n",
        "        text = EMOJI_RE.sub(\"\", text)\n",
        "    text = unidecode(text)           # strip diacritics\n",
        "    return text.lower().strip()\n",
        "\n",
        "# Preview\n",
        "print(raw_ds[0][\"content\"][:120])\n",
        "print(clean_text(raw_ds[0][\"content\"])[:120])"
      ],
      "metadata": {
        "id": "Ih1LbLI5d5a9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SECTION 2â€”Tokenizers **\n",
        "\n",
        "Implement three tokenizers:\n",
        "   1.  Whitespace + spaCy rules (English)\n",
        "   2.  WordPiece (BERT base uncased)\n",
        "   3.  SentencePiece Unigram (train vocab = 8â€¯000)\n",
        " Each should expose a `.encode(text) -> list[int]` method."
      ],
      "metadata": {
        "id": "axgoOF0dd99v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy, torch\n",
        "from transformers import BertTokenizerFast\n",
        "import sentencepiece as spm\n",
        "\n",
        "# 2.1  spaCy whitespace / ruleâ€‘based ------------------------------------------\n",
        "print(\"Loading spaCyâ€¦\")\n",
        "_nlp = spacy.blank(\"en\")\n",
        "\n",
        "class SpacyTokenizer:\n",
        "    def encode(self, text):\n",
        "        return [t.text for t in _nlp.tokenizer(text)]\n",
        "\n",
        "spacy_tok = SpacyTokenizer()\n",
        "\n",
        "# 2.2  WordPiece from BERT -----------------------------------------------------\n",
        "bert_tok = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# 2.3  SentencePieceÂ Unigram  (train on the *cleaned* corpus) ------------------\n",
        "print(\"Training SentencePiece (8k)â€¦\")\n",
        "TXT = Path(\"reviews_clean.txt\"); TXT.write_text(\"\\n\".join(clean_text(x[\"content\"]) for x in raw_ds), encoding=\"utf-8\")\n",
        "spm.SentencePieceTrainer.Train(input=str(TXT), model_prefix=\"sp\", vocab_size=6803, model_type=\"unigram\", character_coverage=1.0)\n",
        "sp_tok = spm.SentencePieceProcessor(model_file=\"sp.model\")\n",
        "\n",
        "# Helper wrapper for uniform API\n",
        "class SPUnigram:\n",
        "    def encode(self, text):\n",
        "        return sp_tok.encode(text)\n",
        "\n",
        "sp_unigram_tok = SPUnigram()"
      ],
      "metadata": {
        "id": "M3GAhc23eKgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SECTION 3â€”Coverage Metrics ** (## TODO)\n",
        "For each tokeniser compute:\n",
        "   * vocab_size             -> len(dictionary)\n",
        "   * OOV rate               -> % tokens not in vocab (spaCy + WordPiece only)\n",
        "   * avg sequence length    -> mean tokens per review\n",
        " Fill the results dict and print nicely."
      ],
      "metadata": {
        "id": "4eGvBWAgeVzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statistics import mean\n",
        "\n",
        "def metrics(tok, name):\n",
        "    lengths, oov = [], 0\n",
        "    all_tokens = [] # To collect tokens for spaCy vocab size\n",
        "    for ex in raw_ds:\n",
        "        ids = tok.encode(clean_text(ex[\"content\"]))\n",
        "        lengths.append(len(ids))\n",
        "        if name == \"spacy\":\n",
        "            all_tokens.extend(ids) # Collect tokens for spaCy\n",
        "        # WordPiece has id 100 for [UNK]; spaCy uses strings so count missing\n",
        "        if name == \"wordpiece\":\n",
        "            oov += ids.count(100)\n",
        "\n",
        "    vocab_size = None\n",
        "    if name == \"spacy\":\n",
        "        vocab_size = len(set(all_tokens)) # Calculate spaCy vocab size\n",
        "    elif name == \"wordpiece\":\n",
        "         vocab_size = tok.vocab_size\n",
        "    elif name == \"sentencepiece\":\n",
        "         vocab_size = tok.get_piece_size() # Access the underlying sp_tok object\n",
        "\n",
        "    return {\n",
        "        \"tokeniser\": name,\n",
        "        \"vocab_size\": vocab_size,\n",
        "        \"oov_rate\": oov / max(1, sum(lengths)) if name == \"wordpiece\" else \"N/A\",\n",
        "        \"avg_len\": mean(lengths)\n",
        "    }\n",
        "\n",
        "# ## TODO: compute metrics for all three tokenisers ---------------------------\n",
        "results = []\n",
        "results.append(metrics(spacy_tok,        \"spacy\"))      # TODO adjust vocab_size logic if desired\n",
        "results.append(metrics(bert_tok,        \"wordpiece\"))\n",
        "results.append(metrics(sp_tok,  \"sentencepiece\")) # Pass the sp_tok object directly\n",
        "\n",
        "print(json.dumps(results, indent=2))"
      ],
      "metadata": {
        "id": "gf6EsT1Ueh6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SECTIONÂ 4Â â€”Â Qualitative check**"
      ],
      "metadata": {
        "id": "nJSdxApCepiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_SENT = \"Iâ€™m LOVING â€˜CafÃ© naÃ¯veâ€™ in 2025!! ðŸ˜ŠðŸ”¥\"\n",
        "print(\"\\nTest sentence:\", TEST_SENT)\n",
        "for name, tok in [(\"spaCy\", spacy_tok), (\"WordPiece\", bert_tok), (\"SentencePiece\", sp_unigram_tok)]:\n",
        "    print(f\"\\n{name} tokens â†’\", tok.encode(TEST_SENT))"
      ],
      "metadata": {
        "id": "uF2eVEMIetna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "f-_7wwadexNx"
      }
    }
  ]
}