{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Week 1 · Lesson 2 — Starter Notebook: Static Word Embeddings**\n"
      ],
      "metadata": {
        "id": "-eqsWbJCkXkR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook you will build two flavours of 200‑dimensional word vectors on a tiny slice of Wikipedia (≈ 10 MB).\n",
        "\n",
        "1) Count‑based:  PPMI  →  Truncated SVD\n",
        "\n",
        "2) Prediction‑based:  Skip‑Gram with negative sampling\n",
        "\n",
        "Fill every section marked `## TODO` and run all cells before submission."
      ],
      "metadata": {
        "id": "IhfFvjXUkcXU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SECTION 0 ─ Imports & Dataset**"
      ],
      "metadata": {
        "id": "aRf_xMTek6XJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6j4UqRX0kW2P",
        "outputId": "04ca475f-f05d-48a6-d519-8deeb9512d28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw corpus size: 1.115394 MB\n"
          ]
        }
      ],
      "source": [
        "!pip -q install nltk gensim scikit-learn tqdm --upgrade\n",
        "\n",
        "import nltk, re, json, math, random, itertools\n",
        "from collections import Counter, defaultdict\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Download a miniature Wiki dump (≈ 10 MB plain text).\n",
        "!wget -q https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O tiny_wiki.txt\n",
        "\n",
        "RAW_PATH = Path('tiny_wiki.txt')\n",
        "print(\"Raw corpus size:\", RAW_PATH.stat().st_size / 1e6, \"MB\")\n",
        "\n",
        "# from datasets import load_dataset\n",
        "# raw_ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "# TEXT   = \"\\n\".join(raw_ds[\"text\"])\n",
        "# print(TEXT)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SECTION 1 ─ Tokenisation helper (✅ implemented for you)**"
      ],
      "metadata": {
        "id": "tCqixDSulArS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TOKEN_RE = re.compile(r\"[A-Za-z']+\")\n",
        "\n",
        "def sent_tokenise(text: str):\n",
        "    for sent in nltk.sent_tokenize(text):\n",
        "        yield [tok.lower() for tok in TOKEN_RE.findall(sent)]\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "sentences = list(sent_tokenise(RAW_PATH.read_text(encoding='utf-8')))\n",
        "print(\"Example sentence tokens:\", sentences[0][:12])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSUVkzzslEDa",
        "outputId": "93980061-df54-4e3b-d510-b4fdbf34ef4d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example sentence tokens: ['first', 'citizen', 'before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SECTION 2 ─ Build 5k vocabulary ** (## TODO)\n",
        "\n",
        "Fill: build a frequency counter, keep the 5 000 most common tokens, map them to indices.  Use index 0 for <UNK>."
      ],
      "metadata": {
        "id": "731OJiLslMR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 5000\n",
        "UNK = \"<unk>\"\n",
        "\n",
        "# ## TODO ---------------------------------------------------------------------\n",
        "word_freq = Counter()\n",
        "for sent in sentences:\n",
        "    word_freq.update(sent)\n",
        "\n",
        "most_common = [w for w, _ in word_freq.most_common(VOCAB_SIZE - 1)]\n",
        "idx2word = [UNK] + most_common\n",
        "word2idx = {w: i for i, w in enumerate(idx2word)}\n",
        "print(\"Vocab built, size:\", len(idx2word))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-jzi5PKlW7U",
        "outputId": "3924c1c3-4f41-449e-ecfc-adff459cbf56"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab built, size: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SECTION 3 ─ ±5 window co‑occurrence counts**"
      ],
      "metadata": {
        "id": "Fgk8fqtelaZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WINDOW = 5\n",
        "cooc = defaultdict(Counter)\n",
        "\n",
        "# ## TODO ---------------------------------------------------------------------\n",
        "for sent in sentences:\n",
        "    indices = [word2idx.get(w, 0) for w in sent]\n",
        "    for i, center in enumerate(indices):\n",
        "        start = max(0, i - WINDOW)\n",
        "        end   = min(len(indices), i + WINDOW + 1)\n",
        "        for j in range(start, end):\n",
        "            if i == j: continue\n",
        "            context = indices[j]\n",
        "            cooc[center][context] += 1\n",
        "print(\"Center vocab with at least one context:\", len(cooc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHqSIdBUlkNq",
        "outputId": "580c1027-47f8-4341-ea77-85cc3c5f70f6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Center vocab with at least one context: 4999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SECTION 4 ─ PPMI matrix  (## TODO) **"
      ],
      "metadata": {
        "id": "9thEVKAjln1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Computing PPMI… (this may take 1‑2 min)\")\n",
        "# Precompute totals\n",
        "sum_all = sum(sum(ct.values()) for ct in cooc.values())\n",
        "row_tot = np.zeros(VOCAB_SIZE)\n",
        "col_tot = np.zeros(VOCAB_SIZE)\n",
        "for i, ctxs in cooc.items():\n",
        "    row_tot[i] = sum(ctxs.values())\n",
        "    for j, c in ctxs.items():\n",
        "        col_tot[j] += c\n",
        "\n",
        "# Build sparse tuples (i, j, ppmi)\n",
        "rows, cols, vals = [], [], []\n",
        "for i, ctxs in cooc.items():\n",
        "    for j, c in ctxs.items():\n",
        "        p_ij = c / sum_all\n",
        "        p_i  = row_tot[i] / sum_all\n",
        "        p_j  = col_tot[j] / sum_all\n",
        "        ppmi = max(0.0, math.log2(p_ij / (p_i * p_j)))\n",
        "        if ppmi > 0:\n",
        "            rows.append(i); cols.append(j); vals.append(ppmi)\n",
        "print(\"PPMI non‑zeros:\", len(vals))\n",
        "\n",
        "# Build sparse ←→ dense matrix (dense for SVD on 5k x 5k is OK)\n",
        "PPMI = np.zeros((VOCAB_SIZE, VOCAB_SIZE), dtype=np.float32)\n",
        "PPMI[rows, cols] = vals"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jV9rticflq4d",
        "outputId": "57ea8ab5-6a27-46f1-dc69-93989e7b80e9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing PPMI… (this may take 1‑2 min)\n",
            "PPMI non‑zeros: 455548\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SECTION 5 ─ Truncated SVD (200‑d)  (## TODO)**"
      ],
      "metadata": {
        "id": "AHXxP9jslxIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Running SVD…\")\n",
        "SVD_DIM = 200\n",
        "svd = TruncatedSVD(n_components=SVD_DIM, random_state=42)\n",
        "count_vecs = svd.fit_transform(PPMI)\n",
        "print(\"Count‑based vectors shape:\", count_vecs.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMBXtuqvl3oO",
        "outputId": "53f752fb-f6b2-488c-d855-8456fb1a5673"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running SVD…\n",
            "Count‑based vectors shape: (5000, 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SECTION 6 ─ Skip‑Gram training (gensim)  (✅ code provided)**"
      ],
      "metadata": {
        "id": "xxxOmkEhl7KX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Skip‑Gram model…\")\n",
        "sg_sentences = [[w for w in sent if w in word2idx] for sent in sentences]\n",
        "sg_model = Word2Vec(sg_sentences, vector_size=200, window=5, min_count=1,\n",
        "                    sg=1, negative=5, epochs=5, workers=4, seed=42)\n",
        "sg_vecs = np.zeros_like(count_vecs)\n",
        "for w, i in word2idx.items():\n",
        "    sg_vecs[i] = sg_model.wv[w] if w in sg_model.wv else np.zeros(SVD_DIM)\n",
        "print(\"Skip‑Gram vectors ready.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZtg5vs3l-gr",
        "outputId": "f4b20809-a168-4730-b579-6527b5da9158"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Skip‑Gram model…\n",
            "Skip‑Gram vectors ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SECTION 7 ─ Helper: nearest neighbours + analogy**"
      ],
      "metadata": {
        "id": "dHyfV7AUmDFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nn(word: str, vecs: np.ndarray, topk: int = 10):\n",
        "    \"\"\"Return nearest-neighbour tokens by cosine similarity.\"\"\"\n",
        "    if word not in word2idx:                    # word unseen in vocab\n",
        "        return [\"<unk>\"]\n",
        "    idx = word2idx[word]\n",
        "    w_vec = vecs[idx]\n",
        "    if not w_vec.any():                        # zero vector -> no info\n",
        "        return [\"<zero-vector>\"]\n",
        "    # cosine similarity (eps avoids /0)\n",
        "    sims = vecs @ w_vec / (\n",
        "        np.linalg.norm(vecs, axis=1) * np.linalg.norm(w_vec) + 1e-9\n",
        "    )\n",
        "    best = sims.argsort()[::-1]                # descending\n",
        "    out  = [idx2word[i] for i in best if i != idx][:topk]\n",
        "    return out or [\"<none>\"]\n",
        "\n",
        "def analogy(a, b, c, vecs):\n",
        "    \"\"\"Return d such that b-a+c ≈ d.\"\"\"\n",
        "    for w in (a, b, c):\n",
        "        if w not in word2idx or not vecs[word2idx[w]].any():\n",
        "            return \"<missing-vector>\"\n",
        "    vec = vecs[word2idx[b]] - vecs[word2idx[a]] + vecs[word2idx[c]]\n",
        "    sims = vecs @ vec / (\n",
        "        np.linalg.norm(vecs, axis=1) * np.linalg.norm(vec) + 1e-9\n",
        "    )\n",
        "    for idx in sims.argsort()[::-1]:\n",
        "        if idx not in (word2idx[a], word2idx[b], word2idx[c]):\n",
        "            return idx2word[idx]\n",
        "    return \"<none>\""
      ],
      "metadata": {
        "id": "w6YC7UKCmGSk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SECTION 8 ─ Evaluation  (## TODO)**"
      ],
      "metadata": {
        "id": "8RLQtXGgmJOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Nearest neighbours for cat, economy, happy\\n\")\n",
        "for name, vecs in [(\"PPMI–SVD\", count_vecs), (\"Skip‑Gram\", sg_vecs)]:\n",
        "    print(name, \"— cat →\", nn(\"cat\", vecs))\n",
        "    print(name, \"— economy →\", nn(\"economy\", vecs))\n",
        "    print(name, \"— happy →\", nn(\"happy\", vecs))\n",
        "\n",
        "print(\"\\nAnalogy king - man + woman → ?\\n\")\n",
        "for name, vecs in [(\"PPMI–SVD\", count_vecs), (\"Skip‑Gram\", sg_vecs)]:\n",
        "    print(name, \"→\", analogy(\"man\", \"king\", \"woman\", vecs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQOs_0uZmMCw",
        "outputId": "2d36423f-046c-4ded-ab81-8d8ef653cbe1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nearest neighbours for cat, economy, happy\n",
            "\n",
            "PPMI–SVD — cat → ['mouse', \"shunn'd\", 'dog', 'scratch', 'budge', 'every', \"ne'er\", 'unworthy', \"man's\", 'little']\n",
            "PPMI–SVD — economy → ['<unk>']\n",
            "PPMI–SVD — happy → ['befal', 'days', 'joyful', 'watchful', 'doubt', 'victory', 'blessings', 'girl', \"'mongst\", 'enrich']\n",
            "Skip‑Gram — cat → ['toys', 'non', 'foreign', 'fold', 'silk', 'wisest', 'wither', \"kings'\", 'criminal', 'din']\n",
            "Skip‑Gram — economy → ['<unk>']\n",
            "Skip‑Gram — happy → [\"say'st\", 'tall', 'prospero', 'fond', 'coward', 'widow', \"know'st\", 'tut', 'comest', 'woman']\n",
            "\n",
            "Analogy king - man + woman → ?\n",
            "\n",
            "PPMI–SVD → richard\n",
            "Skip‑Gram → bolingbroke\n"
          ]
        }
      ]
    }
  ]
}